# Activation Functions
앞선 강의에서도 언급했던 것처럼, input이 들어오면 가중치와 곱해지고, 비선형 함수인 활성함수를 거쳐 해당 데이터의 활성화여부를 결정해준다고 배웠다. 
활성화 함수에는 여러가지가 있고, 각 활성함수의 문제점과 이를 어떻게 개선해나갔는지에 대해 알아볼 것이다. 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/f78e4ffc-7678-46ff-9ee6-d6d16bd09f59" height="70%" width="70%"></p>

활성함수는 기본적으로 input을 특정 범위의 출력으로 변환해주는 단순한 기능도 하지만, output에 비선형성을 부여하기도 한다. 
활성함수는 미분이 가능해야 하며 그렇지 않다면 역전파를 실패하게 된다. 

비선형성이 필요한 이유는 아래와 같다. 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/3f5148ac-27aa-4fd4-b1e6-37b2b9fe3b42" height="70%" width="70%"></p>

위의 그림과 같이 circle 또는 elliptical 같은 non-linearity data가 있다고 가정하고, data point가 positive class or nagative class에 속하는지 분류하는 작업을 생각해보면 이런 경우엔 선형 모델을 적용하는 것이 불가능하다. 


### Sigmoid 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/69847ed5-8d1a-4143-9f43-9ff25654ef89" height="40%" width="40%"></p>

$$\sigma(x) = \frac{1}{1+e^{-x}}$$

Sigmoid의 문제는 아래와 같은 것들이 있다. 

#### Vanishing Gradient
첫 번째는 Vanishing Gradient 문제이다. 
Sigmoid는 범위가 [0, 1]인데 0의 가까운 값에서만 simgoid가 active된다고 볼 수 있다. 
그래프를 보면 알 수 있듯이 만약 W가 10이거나 -10인 경우(매우 작거나, 매우 클 때)에는 1 또는 0의 값으로 수렴하게 된다. 
이렇게 되면 도함수가 0에 가까워지며 작아지므로 error가 sigmoid activated neural networks에서 back propagation하는 동안, 기울기 저하가 발생하고, gradient가 사라지게 된다. 
초기 레이어에 대한 gradient 값이 줄어들고 해당 레이어는 제대로 학습할 수 없게된다. 
즉, 네트워크의 깊이와 값이 0으로 이동하는 활성화로 인해 기울기가 사라지는 경향이 있다. 

#### Not zero-centered
sigmoid의 결과는 0 중심이 아니다.  모든 값은 0 이상의 값을 가져서 slow convergence를 가져오게 된다. 
Zero-centered란 그래프의 중심이 0인 상태로, 함숫값이 음수 혹은 양수에만 치우쳐 존재하지 않고 실수 전체에서 고르게 나타나는 형태를 의미한다. 하지만 시그모이드는 위의 그래프에서도 볼 수 있듯이 함숫값이 항상 0~1이기 때문에 함숫값이 양수만 존재한다.

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/78ec8259-443a-4151-a9ca-4cb6a9df1583" height="70%" width="70%"></p>

이것의 문제점은 위 사진에서 볼 수 있다. 위의 예시를 통해 이것의 문제점을 알아보자. Neural networks에서 input은 예전 layer의 결과값이라고 생각하면 된다. (activation function을 통과한 결과 값을 다음 layer에 넘기기 때문) 그런데 시그모이드 함수의 결과값은 항상 양수이기 때문에 시그모이드 함수를 한번 거친 이후로는 이 input 값이 항상 양수가 된다. 이렇게 되면 backpropagation을 할 때 문제가 생긴다. backpropagation을 하게 되면 chain rule을 사용하는데, $\frac{dL}{dw} = \frac{dL}{df} * \frac{df}{dw}$ 의 계산 과정을 거칠 때 $]frac{df}{dw} = x$이므로 $\frac{dL}{dw} = \frac{dL}{df} * x$ 가 된다. 이렇게 되면 input값인 x(local gradient)가 항상 양수이기 때문에 $\frac{dL}{dw}$ 와 $\frac{dL}{df}$의 부호는 같을 수밖에 없다.

w가 2차원이라고 하면 좌표 평면에서 부호를 따져봤을 때 부호가 같은 부분은 1,3사분면이다. 따라서 가상 최적 w는 파란색 벡터임에도 불구하고, 빨간색 벡터처럼 지그재그의 형태로 기울기가 업데이트(학습) 될 수밖에 없다. 그렇기 때문에 zero-centered가 필요하다. input x의 부호가 다양해야 기울기를 업데이트 할 때 적절한 방향으로 진행할 수 있기 때문이다.


#### Compute expensive
보통 지수 함수 $exp()$의 경우 연산이 굉장히 커서, 성능이 저하 된다고 볼 수 있다.  


### tanh

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/76404bc1-1928-4635-b9d9-0d469ad07b2f" height="40%" width="40%"></p>

$$tanh(x)$$

$tanh$의 경우 data가 0을 중심으로 한다. 이는 입력 데이터의 평균이 0 근처에 있음을 의미하고, Not zero-centered 문제를 해결할 수 있다. 따라서 zero centered 이지만 saturated 될 때 gradient는 여전히 죽는 문제는 남아있다. 


### ReLU

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/21663338-9508-4f42-a08c-bdd42ae25146" height="40%" width="40%"></p>

$$max(0, x)$$

#### The best activation function
복잡한 계산을 필요로 하지 않기 때문에, 계산 효율 좋다.(sigmoid나 tanh보다 수렴 속도가 약 6배 빠름)
또한 x가 양수이면 saturation 되지 않는다. 

ReLU 활성화 함수를 사용할 때의 이점을 고려하는 또 다른 중요한 속성은 sparsity(희소성)이다. 
일반적으로 대부분의 항목이 0인 행렬을 sparse matrix(희소 행렬)이라고 하며 마찬가지로 일부 가중치가 0인 신경망에서 이와 같은 속성을 원한다. 

sparsity은 종종 더 나은 predictive power와 overfitting/noise가 적은 간결한 모델을 생성한다. 
sparse network에서는 뉴런이 실제로 문제의 의미 있는 측면을 처리할 가능성이 더 큰데, 예를 들어, 이미지에서 사람의 얼굴을 감지하는 모델에는 귀를 식별할 수 있는 뉴런이 있을 수 있으며, 이미지가 얼굴이 아니고 배나 산인 경우 활성화되지 않아야 한다. 

ReLU는 모든 음수 입력에 대해 출력 0을 제공하기 때문에 주어진 단위가 전혀 활성화되지 않아 네트워크가 sparse해질 가능성이 있다. 

#### Exploding Gradient
ReLU 활성화 함수에는 exploding gradient과 같은 몇 가지 문제가 있다. 

exploding gradient는 vanishing gradient의 반대 개념이다. 
큰 오류 gradient가 누적되어서 훈련 중에 신경망 모델 가중치가 매우 크게 업데이트되는 경우에 발생한다. 

또한, 모든 음수 값에 대해 0이 되는 점이 단점이 되기도 한다. 

이 문제를 dead ReLU라고 하며, ReLU neuron이 음수 쪽에 붙어 있고 항상 0을 출력하면 "dying"한다고 본다. 
음수 값에서 ReLU의 gradient 범위도 0이다. neuron이 음수가 되면 다시 살아날 가능성이 거의 없다. 
이러한 뉴런은 input을 구별하는 데 아무런 역할도 하지 않으며 본질적으로 쓸모가 없게된다. 
dying하는 문제는 learning rate가 너무 높거나 negative bias가 클 때 발생하기 쉽다. 


<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/9831c709-6974-45b8-b64a-e69ebb57a288" height="70%" width="70%"></p>

- 초기화를 잘못해서 가중치 평면이 data cloud에서 멀리 떨어진 경우
- Learning rate가 지나치게 높은 경우, 가중치가 날뛰게 되며 ReLU가 데이터의 manifold를 벗어나게 됨
- 학습 다 시켜놓은 네트워크를 살펴보면 10-20%는 dead ReLU가 되어있는데, 이정도는 괜찮다.
- 그림에서 초록빨강 평면은 ReLU의 입력 행렬을 표현한 것
- 그래서 초기화시에 positive biases를 추가해주는 경우 많다. (active 될 확률 높이기)
- 대부분은 zero-bias로 사용

### Leaky ReLU

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/6812503e-bdc4-43df-b2a9-83f199296877" height="40%" width="40%"></p>

$$f(x) = max(0.01x, x)$$

Leaky ReLU는 ReLU Function의 Dying 문제를 극복한 ReLU Activation Function의 확장이다. 
Dying ReLU 문제는 제공된 모든 입력에 대해 비활성화되어 Neural Network의 성능에 영향을 미친다. 
이 문제를 해결하기 위해 ReLU Activation Function과 달리 Negative 입력에 대한 음의 기울기가 작게 조정된 Leaky ReLU가 있다. 

Leaky ReLU의 한계는 복잡한 분류에 사용할 수 없는 선형 곡선을 가지고 있다는 것이다. 

- zero-mean
- 음의 영역에서도 이제 saturation 되지 않음
- dead ReLU도 없음

### PReLU

$$f(x) = max(\alpha x, x)$$

- Leaky ReLU와 비슷하지만 기울기 alpha (파라미터)로 결정됨


### ELU

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/8f195da2-e21f-4eb4-8aeb-fc6807dc0c7a" height="40%" width="40%"></p>

$$
f(x)=
\begin{cases}
x & if \ x > 0 \\
\alpha(exp(x)-1 & if \ x \le 0 \\ 
\end{cases}
$$

- zero-mean에 가까운 출력값
- 음에서 saturation. 하지만 saturation이 노이즈에 강인하다고 생각
- dying ReLU문제 극복(음수 값의 기울기가 0이 아니라서)
- ReLU, sigmoid, Hyperbolic Tangent보다 높은 accuracy를 갖는다.

ReLU와 달리 ELU는 음숫값을 가지므로 함수의 평균이 0으로 이동하고, 이 점에서 더 빠르게 convergence한다고 주장된다.
ELU 함수는 지수함수여서 계산 속도는 느리지만 더 빠른 convergence로 빠르게 학습되는 원리이다. 


### Maxout Neuron

$$max(w_1^Tx + b_1, w_2^Tx + b_2)$$

- 기본형식을 정의하지 않음
- 두개의 선형함수 중 큰 값을 선택 -> ReLU와 leaky RELU의 일반화 버전
- 선형이기에 saturation안되고 gradient 안죽음
- W1, W2 때문에 파라미터 수 두배됨

여러 선형 함수를 사용하여 함수를 근사하는 것을 piece-wise linear approximation(PWL) 라고한다. 

### TLDR: In practice:
- Use **ReLU**. Be careful with your learning rates
- Try out **Leaky ReLU / Maxout / ELU**
- Try out tanh but don’t expect much
- **Don’t** use sigmoid



# Data Preprocessing

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/97d5d794-cd77-441c-84c7-9294dce58f2e" height="70%" width="70%"></p>

- zero-mean으로 만들고 표준편차로 normalize
- 이미지의 경우 스케일이 어느정도 맞춰져 있어서 zero-mean만 해준다.

해주는 이유는 앞서 입력이 전부 positive한 경우를 방지하는 것과 같음. 학습최적화를 위한 것. 
평균값은 전체 training data에서 계산하여 test 데이터에도 동일하게 적용. 
RGB별로 각각 평균을 내는 경우도 있다.

## Weight Initialization
모든 파라미터를 0으로 설정한다면? (W=0)  
- 모든 뉴런이 같은일을 한다
- 모든 가중치가 똑같은 값으로 업데이트됨
- 모든 가중치를 동일하게 초기화시키면 symmetry breaking이 일어날 수 없다.
- 서로 다른 loss를 가질 수 있으나 많은 뉴런들이 동일한 가중치로 연결되어 있을것이며, 모든 뉴런이 같은 방식으로 업데이트 될 것이다.

임의의 작은 값으로 초기화하기
```py
W = 0.01 * np.random.randn(D, H)
```

- 초기 W를 표준정규분포에서 샘플링 한다.
- 딥뉴럴네트워크에서 여전히 문제가 생길 수 있다.

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/96e2a4cc-a5b1-495d-86eb-eac684a81e20" height="70%" width="70%"></p>

위의 그래프처럼 출력깂이 점점 0에 가까워짐을 알 수 있다. 이건 우리가 원하는 것이 아니다. 
즉 입력 값이 0에 수렴하고, 이는 gradient를 매우 작게 만든다. 결국 업뎃이 잘 안일어나게 된다. 

좀 더 큰 값으로 초기화하기  
- saturation되고 gradient 0

그래서 고안한 것이 Xavier Initalization - Glorot(2010)

```py
W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in) # layer initialization
```

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/9626925d-fd83-49de-9992-1f0ab73de3c5" height="70%" width="70%"></p>

- stadard gaussian으로 뽑은 값을 입력의 수로 스케일링 해준다.
- 입출력의 분산을 맞춰주는 것
- 입력 수가 작으면 더 작은 값으로 나누고 좀 더 큰 값을 얻는다. 더 큰 가중치가 필요하다. 그래야 출력의 분산 만큼 큰 값을 얻을 수 있다.
- 각 레이어의 입력이 Unit gaussian이게 된다.
- 하지만 ReLU를 쓰면 출력 분산의 절반을 날려버리는 거라서 잘 작동하지 않음. 값이 너무 작아짐. 결국 비활성됨.
- 이를 위해 추가적으로 2를 더 나눠주어 절반이 없어졌다는 사실을 고려하면 꽤 잘작동한다. (He Initialization - Reference 참고) 

  


# Reference 
https://moordo91.tistory.com/32   
https://velog.io/@cha-suyeon/DL-%EA%B0%80%EC%A4%91%EC%B9%98-%EC%B4%88%EA%B8%B0%ED%99%94Weight-Initialization-  
https://velog.io/@cha-suyeon/cs231n-6%EA%B0%95-%EC%A0%95%EB%A6%AC-Training-Neural-Networks-I      
https://say-young.tistory.com/entry/CS231n-Lecture-6-Training-Neural-Networks-I




