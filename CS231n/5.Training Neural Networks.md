# Activation Functions
앞선 강의에서도 언급했던 것처럼, input이 들어오면 가중치와 곱해지고, 비선형 함수인 활성함수를 거쳐 해당 데이터의 활성화여부를 결정해준다고 배웠다. 
활성화 함수에는 여러가지가 있고, 각 활성함수의 문제점과 이를 어떻게 개선해나갔는지에 대해 알아볼 것이다. 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/f78e4ffc-7678-46ff-9ee6-d6d16bd09f59" height="70%" width="70%"></p>

### Sigmoid 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/d21b181f-41a8-455e-8417-eda1d9d4fc65" height="30%" width="30%"></p>

$$\sigma(x) = \frac{1}{1+e^{-x}}$$

#### problem1
- 음/양의 큰값에서 Saturation되는 것이 gradient를 없앤다.
- x가 0에 가까운건 잘 동작한다.

#### problem2
- 출력이 zero centered가 아니다.
- 만약 x가 항상 양수일 떄, w의 gradient는 시그모이드 upstream gradient와 부호가 항상 같게 된다. 이는 W로 하여금 모두 양의 방향이나, 모두 음의 방향으로밖에 업데이트가 되지 못하게 하기 때문에, zig zag path를 따르게 되며, 비효율적이다.

#### problem3
- exp() 가 연산 cost가 비싸다.

### tanh

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/b97b9bd2-d29e-47df-bb17-12090dd99d9e" height="30%" width="30%"></p>

$$tanh(x)$$

zero-centered 문제는 해결되었으나, saturation 문제는 여전히 존재한다. 


### ReLU

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/06519ff8-28c4-47c9-a316-6f28e91b088a" height="30%" width="30%"></p>

$$max(0, x)$$

- x가 양수이면 saturation 되지 않는다.
- 계산 효율 좋음( sigmoid나 tanh보다 수렴 속도가 약 6배 빠름)
- 생물학적 타당성

#### problem
- zero- centered가 아님
- 음의 영역에서는 saturation (x=0에서도 gradient 0)
- gradient의 절반을 죽인다 -> dead ReLU라고 함

#### Dead ReLU

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/9831c709-6974-45b8-b64a-e69ebb57a288" height="70%" width="70%"></p>

- 초기화를 잘못해서 가중치 평면이 data cloud에서 멀리 떨어진 경우
- Learning rate가 지나치게 높은 경우, 가중치가 날뛰게 되며 ReLU가 데이터의 manifold를 벗어나게 됨
- 학습 다 시켜놓은 네트워크를 살펴보면 10-20%는 dead ReLU가 되어있는데, 이정도는 괜찮다.
- 그림에서 초록빨강 평면은 ReLU의 입력 행렬을 표현한 것
- 그래서 초기화시에 positive biases를 추가해주는 경우 많다. (active 될 확률 높이기)
- 대부분은 zero-bias로 사용

### Leaky ReLU

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/2885ff5e-a605-4cf8-8e50-2c32edd243ea" height="30%" width="30%"></p>

$$f(x) = max(0.01x, x)$$

- zero-mean
- 음의 영역에서도 이제 saturation 되지 않음
- dead ReLU도 없음

### PReLU

$$f(x) = max(\alpha x, x)$$

- Leaky ReLU와 비슷하지만 기울기 alpha (파라미터)로 결정됨


### ELU

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/311cff2c-3bb2-4a14-8ad2-a2947373cae6" height="30%" width="30%"></p>

$$
f(x)=
\begin{cases}
x & if \ x > 0 \\
\alpha(exp(x)-1 & if \ x \le 0 \\ 
\end{cases}
$$

- zero-mean에 가까운 출력값
- 음에서 saturation. 하지만 saturation이 노이즈에 강인하다고 생각


### Maxout Neuron

$$max(w_1^Tx + b_1, w_2^Tx + b_2)$$

- 기본형식을 정의하지 않음
- 두개의 선형함수 중 큰 값을 선택 -> ReLU와 leaky RELU의 일반화 버전
- 선형이기에 saturation안되고 gradient 안죽음
- W1, W2 때문에 파라미터 수 두배됨


### TLDR: In practice:
- Use **ReLU**. Be careful with your learning rates
- Try out **Leaky ReLU / Maxout / ELU**
- Try out tanh but don’t expect much
- **Don’t** use sigmoid



# Data Preprocessing

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/97d5d794-cd77-441c-84c7-9294dce58f2e" height="70%" width="70%"></p>

- zero-mean으로 만들고 표준편차로 normalize
- 이미지의 경우 스케일이 어느정도 맞춰져 있어서 zero-mean만 해준다.

해주는 이유는 앞서 입력이 전부 positive한 경우를 방지하는 것과 같음. 학습최적화를 위한 것. 
평균값은 전체 training data에서 계산하여 test 데이터에도 동일하게 적용. 
RGB별로 각각 평균을 내는 경우도 있다.

## Weight Initialization
모든 파라미터를 0으로 설정한다면? (W=0)  
- 모든 뉴런이 같은일을 한다
- 모든 가중치가 똑같은 값으로 업데이트됨
- 모든 가중치를 동일하게 초기화시키면 symmetry breaking이 일어날 수 없다.
- 서로 다른 loss를 가질 수 있으나 많은 뉴런들이 동일한 가중치로 연결되어 있을것이며, 모든 뉴런이 같은 방식으로 업데이트 될 것이다.

임의의 작은 값으로 초기화하기
```py
W = 0.01 * np.random.randn(D, H)
```

- 초기 W를 표준정규분포에서 샘플링 한다.
- 딥뉴럴네트워크에서 여전히 문제가 생길 수 있다.

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/96e2a4cc-a5b1-495d-86eb-eac684a81e20" height="70%" width="70%"></p>

위의 그래프처럼 출력깂이 점점 0에 가까워짐을 알 수 있다. 이건 우리가 원하는 것이 아니다. 
즉 입력 값이 0에 수렴하고, 이는 gradient를 매우 작게 만든다. 결국 업뎃이 잘 안일어나게 된다. 

좀 더 큰 값으로 초기화하기  
- saturation되고 gradient 0

그래서 고안한 것이 Xavier Initalization - Glorot(2010)

```py
W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in) # layer initialization
```

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/9626925d-fd83-49de-9992-1f0ab73de3c5" height="70%" width="70%"></p>

- stadard gaussian으로 뽑은 값을 입력의 수로 스케일링 해준다.
- 입출력의 분산을 맞춰주는 것
- 입력 수가 작으면 더 작은 값으로 나누고 좀 더 큰 값을 얻는다. 더 큰 가중치가 필요하다. 그래야 출력의 분산 만큼 큰 값을 얻을 수 있다.
- 각 레이어의 입력이 Unit gaussian이게 된다.
- 하지만 ReLU를 쓰면 출력 분산의 절반을 날려버리는 거라서 잘 작동하지 않음. 값이 너무 작아짐. 결국 비활성됨.
- 이를 위해 추가적으로 2를 더 나눠주어 절반이 없어졌다는 사실을 고려하면 꽤 잘작동한다. (He Initialization - Reference 참고) 

  


# Reference 
https://moordo91.tistory.com/32   
https://velog.io/@cha-suyeon/DL-%EA%B0%80%EC%A4%91%EC%B9%98-%EC%B4%88%EA%B8%B0%ED%99%94Weight-Initialization-  
https://velog.io/@cha-suyeon/cs231n-6%EA%B0%95-%EC%A0%95%EB%A6%AC-Training-Neural-Networks-I   
  




