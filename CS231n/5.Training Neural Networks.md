# Activation Functions
앞선 강의에서도 언급했던 것처럼, input이 들어오면 가중치와 곱해지고, 비선형 함수인 활성함수를 거쳐 해당 데이터의 활성화여부를 결정해준다고 배웠다. 
활성화 함수에는 여러가지가 있고, 각 활성함수의 문제점과 이를 어떻게 개선해나갔는지에 대해 알아볼 것이다. 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/f78e4ffc-7678-46ff-9ee6-d6d16bd09f59" height="70%" width="70%"></p>

### Sigmoid 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/d21b181f-41a8-455e-8417-eda1d9d4fc65" height="30%" width="30%"></p>

$$\sigma(x) = \frac{1}{1+e^{-x}}$$

#### problem1
- 음/양의 큰값에서 Saturation되는 것이 gradient를 없앤다.
- x가 0에 가까운건 잘 동작한다.

#### problem2
- 출력이 zero centered가 아니다.
- 만약 x가 항상 양수일 떄, w의 gradient는 시그모이드 upstream gradient와 부호가 항상 같게 된다. 이는 W로 하여금 모두 양의 방향이나, 모두 음의 방향으로밖에 업데이트가 되지 못하게 하기 때문에, zig zag path를 따르게 되며, 비효율적이다.

#### problem3
- exp() 가 연산 cost가 비싸다.

### tanh

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/b97b9bd2-d29e-47df-bb17-12090dd99d9e" height="30%" width="30%"></p>

$$tanh(x)$$

zero-centered 문제는 해결되었으나, saturation 문제는 여전히 존재한다. 


### ReLU

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/06519ff8-28c4-47c9-a316-6f28e91b088a" height="30%" width="30%"></p>

$$max(0, x)$$

- x가 양수이면 saturation 되지 않는다.
- 계산 효율 좋음( sigmoid나 tanh보다 수렴 속도가 약 6배 빠름)
- 생물학적 타당성

#### problem
- zero- centered가 아님
- 음의 영역에서는 saturation (x=0에서도 gradient 0)
- gradient의 절반을 죽인다 -> dead ReLU라고 함

#### Dead ReLU

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/9831c709-6974-45b8-b64a-e69ebb57a288" height="70%" width="70%"></p>

- 초기화를 잘못해서 가중치 평면이 data cloud에서 멀리 떨어진 경우
- Learning rate가 지나치게 높은 경우, 가중치가 날뛰게 되며 ReLU가 데이터의 manifold를 벗어나게 됨
- 학습 다 시켜놓은 네트워크를 살펴보면 10-20%는 dead ReLU가 되어있는데, 이정도는 괜찮다.
- 그림에서 초록빨강 평면은 ReLU의 입력 행렬을 표현한 것
- 그래서 초기화시에 positive biases를 추가해주는 경우 많다. (active 될 확률 높이기)
- 대부분은 zero-bias로 사용

### Leaky ReLU

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/2885ff5e-a605-4cf8-8e50-2c32edd243ea" height="30%" width="30%"></p>

$$f(x) = max(0.01x, x)

- zero-mean
- 음의 영역에서도 이제 saturation 되지 않음
- dead ReLU도 없음

### PReLU

$$f(x) = max(\alpha x, x)$$

- Leaky ReLU와 비슷하지만 기울기 alpha (파라미터)로 결정됨






