# Computer Vision Tasks

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/30b6bccc-633a-4d16-b70b-01da3ad1ddc2" height="80%" width="80%"></p>

Detection 부터는 여러 object가 있고 각각의 object를 찾아낸다. Segmentation은 각 object를 형상대로 구분하게 된다. 

## Classification + Localization 
Classification같은 경우 C개의 class가 있을 때 input으로 이미지를 받고 output으로 class label을 준다. 평가 지표는 정확도가 된다. 
Localization은 input으로 이미지가 들어오면 output은 label이 아니라 box이다. x, y 좌표 기준 width, height 이렇게 4개의 값(x, y, w, h)을 output으로 준다. 평가 지표는 Intersection Over Union(IOU)로 box와 object가 겹치는 비율이다. 

따라서 결국 Classification과 Localization을 통합하면 label과 box 위치 정보 모두 얻게 되는 것이다. 
Localization에는 크게 2가지 방법이 있다. 

### Idea #1 : Localization as Regression

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/1a9b23b1-7ed2-412f-9308-7e32fda09763" height="80%" width="80%"></p>

첫 번째 방법은 Localization을 Regression으로 간주하는 것이다. 이 방법은 1개 또는 n개의 object를 Localization할 때 굳이 detection을 사용하지 않아도 된다. 방법은 위 사진처럼 이미지를 넣고 결과로 4개 수로 이루어진 좌표와 실제 정답 box 좌표를 비교해서 loss를 구하게 된다. 그리고 이 값을 역전파에서 update해서 학습하는 방식이다. 단계별로 보면 아래와 같다. 

#### Step 1 : Train (or download) a classification model (AlexNet, VGG, GoogleNet)  

#### Step 2 : Attach new fully-connected "regression head" to the network

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/8f0acc9d-cf79-4bbd-a59f-53de9d59b680" height="70%" width="70%"></p>

위 사진 처럼 결과물이 box의 좌표로 오도록 하는 regression head가 추가되었다. 

#### Step 3 : Train the regression head only with SGD and L2 loss
3 단계에서는 추가된 regression head부분에 대해서만 학습을 시켜준다. 

#### Step 4 : At test time use both heads 
마지막 4 단계에서는 Classification head와 regression head 모두를 이용해서 결과를 산출한다. 


### Per-class vs class agnostic regression
regression head에는 2가지 방법이 있다. Per-class의 경우는 class에 특화된 방법이고, class agnostic은 각각의 class와 무관하게 범용적인 방법이라 할 수 있다. class agnostic의 경우 class에 특화되지 않기 때문에 결과로 그냥 하나의 box 4개의 숫자만 나오게 되고, class에 특화된 class specific한 방법은 각 클래스당 한 box로 결과가 나와 총 `4 X C` class의 수에 4를 곱한 숫자가 나오게 된다.
이 두가지 방법은 loss를 구하는데 약간의 차이가 있을 뿐 유사한 방법이다. 

regression head를 적용해야하는 위치 또한 2가지 방법이 있는데 이 2가지 방법 모두 통한다. 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/8d51fb1d-a6d6-4879-a4b1-881b67afd8da" height="80%" width="80%"></p>


첫 번째 방법으로는 마지막 conv layer 뒤에다 붙여주는 방식으로 Overfeat이나 VGG를 사용하는 경우 이 방법을 사용하고, FC layer 뒷 단에 붙여주는 경우도 있다. DeepPose나 R-CNN의 경우 이 방법을 사용하고 어떤 경우든 다 잘 동작한다.  

결론적으로 object의 수가 정해진 경우에는 regression만으로도 잘 동작하게 된다. 이럴 때는 굳이 detection을 쓰지 않아도 된다. 


### Idea #2 : Sliding Window 
Sliding Window는 Regression과 비슷하게 여전히 Classification head와 regression head 두 개로 분리해서 진행하지만, 이미지의 한 군데가 아닌 여러군데를 여러번 돌리고, 최종적으로 합쳐주는 기법이다. 
또한 편의성을 위해 fully-connected layer를 convolutional layer로 변환해서 연산을 진행한다. 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/65d10336-72b3-4392-b7b9-a551e8f7694c" height="80%" width="80%"></p>

위 사진에서 검은색 박스는 Sliding Window가 되고 이 Sliding Window를 통해 위 사진 기준으로는 왼쪽 위를 먼저 보게 된다. 그 다음 regression head에 의해 만들어진 빨간색 박스와 함께 Classification score에 고양이로 분류되는 정도를 계산한다. 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/4e6befdd-3606-43c4-a0b6-1e8aa9115c4d" height="60%" width="60%"></p>

이런식으로 위 사진과 같이 Sliding Window를 계속 움직여 주면서 score를 계산해나간다. 

실제 사용할 때는 수백게의 Sliding Window를 사용하게 될 수도 있는데 Sliding Window를 사용한 연산이 복잡해서 이를 효율적으로 하기 위한 방법이 FC를 convolutional layer로 바꾸는 것이다. 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/7c70144c-3e0b-4697-b7a6-02ae1979107d" height="80%" width="80%"></p>

이렇게 conv layer를 사용하는데, 기존에 FC에서 4096 개의 원소로 구성된 벡터를 벡터가 아닌 또 하나의 convolutional feature map(입력으로부터 커널을 사용하여 합성곱 연산을 통해 나온 결과)으로 생각하는 것이다.  
따라서 1 X 1 차원을 추가하여 conv layer로 만들어버린다. 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/accc489d-8a2a-4958-b7cd-e653d2303c97" height="80%" width="80%"></p>


## Object Detection 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/9095b001-1cd8-4641-bf64-e0a12a420db6" height="80%" width="80%"></p>

Object Detection 한 이미지 내에 불특정 개수의 여러 Object를 인식하는 기술이다. 처음 나온 아이디어는 기존 regression을 활용하는 것인데 문제는 Object Detection은 불특정 개수이기 때문에 고양이가 한 마리가 있을 수도 있고 여러마리가 있을 수도 있다. 따라서 이미지에 따라 도출되는 number의 수가 달라지기 때문에 regression은 적합하지가 않다. 그럼에도 불구하고 나중에 나오는 YOLO라는 모델은 regression을 사용해서 Detection을 하게 된다. 

결국 일반적으로는 regression이 적합하지 않기 때문에 Detection을 Classification으로 간주하는 접근 방법이 나오게 된다. 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/ffedd649-9a07-46b6-8e9d-e57e69318f25" height="160" width="400"><img src="https://github.com/em-1001/AI/assets/80628552/520c3fae-aca2-419b-ae7e-0c3d361d3129" height="160" width="400"></p>

그래서 위 사진과 같이 박스의 위치에 따라 고양이 인지 아님 강아지인지 분류를 하게 된다. 
이렇게 Classification으로 간주했을 때의 문제는 다양한 크기의 윈도우를 가지고 다양한 이미지의 전 영역에 해야하기 때문에 테스트의 수가 매우 많다는 것이다. 하지만 conv net 같은 무거운 classifier가 아닌 이상 그냥 시도하는 것이 해결책이었는데 실제로 이 방법이 잘 통했었다. 

대표적으로 HOG같은 경우 매우 빠른 선형 분류로 이미지를 최대한 다양한 해상도에서 돌려서 잘 분류를 하였다. 
HOG에 대한 후속 연구로는 DPM이 있다. 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/3895d734-a0da-43c1-9363-3efdacc9a950" height="80%" width="80%"></p>

DPM은 여전히 HOG를 기반으로 동작을 하고 특징이 특정 부분부분 사람의 경우 머리 다리 등 부분에 대한 템플릿을 가지고 있어고 이 템플릿들은 각각이 변형된 형태까지 다 기지고 있어서 당시 수준으로는 잘 동작을 하였다. 하지만 나중에 논문으로 나온 것이 이 방법이 결국 이름만 다를 뿐 CNN을 사용하는 것과 유사하다는 것이다. 

이렇듯 Classification으로 간주했을 때 발생하는 두 번째 문제는 CNN과 같은 무거은 classifier를 사용해야 한다는 것이다. 
이런 경우 모든 영역과 scale을 다 보는 것은 연산적으로 매우 무겁다. 
이 문제의 해결책은 모든 영역을 다 보기보다 의심되는 영역만 조사하는 것이다. 그래서 의심되는 영역을 추천한다고 해서 Region Proposal 방식이라고 한다. 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/70e4124a-fe18-4a08-a749-4a4233ca544c" height="80%" width="80%"></p>

그래서 Region Proposal은 결국 object를 포함하고 있을 거 같은 blobby한 부분 즉, 뭉쳐있고, 유사한 색이나 텍스쳐를 가지고 있는 부분을 말하고, 클래스와 무관한 blobby한 부분만 찾는 detector가 된다. 
이미지를 보면 강아지이던 고양이던 class에 신경쓰지 않고 코, 눈 등을 잡아내는 것을 볼 수 있다. 장점은 매우 빠르다는 것이다. 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/5dbb8711-83d7-41cf-9683-9f169501fc9a" height="80%" width="80%"></p>

Region Proposal의 방식은 여러가지가 있는데 대표적으로 Selective Search는 이미지의 픽셀에서 시작해서 색상과 텍스쳐가 유사한 픽셀들끼리 묶게되고 이것들을 알고리즘을 활용해서 merge하여 큰 blob을 만들어 낸다. 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/ef89942c-654b-459a-8b36-02cc7e4f5107" height="80%" width="80%"></p>

Selective Search이외에도 수많은 방법이 있다. 추천되는 방식으로는 EdgeBoxes라고 한다. 

### R-CNN

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/3b888c04-9eee-4f2d-8457-59467634f746" height="70%" width="70%"></p>

결론적으로 지금까지 본 Region Proposal과 CNN을 결합한 것이 R-CNN이다. 
사진을 보면 input image를 받아서 관심있는 지역(box) ROI를 뽑아낸다. 수는 대략 2천개를 뽑아내고, 이렇게 뽑은 각각의 box들은 각기 다른 크기와 위치에 해당된다. 그 다음 이 box들을 Warping하는 작업을 거친다. 일반적으로 CNN에 들어가는 정사각형 크기로 warp를 시켜주고 이렇게 만들어진 각각의 ROI를 CNN으로 학습시킨다. 위 사진에는 Classification head와 regression head로 두 개의 head가 존재하는 것을 볼 수 있는데 Classification head을 이용해서는 SVM을 통해 분류를 하고 regression head에서는 bounding box를 추출해 낸다. 










# Reference
https://www.youtube.com/watch?v=y1dBz6QPxBc&list=PL1Kb3QTCLIVtyOuMgyVgT-OeW0PYXl3j5&index=7  
