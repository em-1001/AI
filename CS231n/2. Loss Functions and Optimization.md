# Loss function
### Multiclass SVM loss
손실함수에도 여러종류가 있으나, 기본적이고 이미지분류에도 성능이 좋은 Multiclass SVM loss부터 살펴보자. 식은 다음과 같다. 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/0788a2cf-adab-4acd-96e6-071e31ca92b8" height="50%" width="50%"></p>

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/0159e5f0-3630-4cb3-92c9-b385ba7b534c" height="40%" width="40%"></p>

$S_j$ : Classifier로부터 나온 예측 값  
$S_{y_i}$ : True 값 

이 개념을 이용해 다음 예제를 풀어보자. 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/3b8fd972-d322-482d-b988-7635826e2f86" height="40%" width="40%"></p>

cat의 Loss를 구해보면 $max(0, 5.1 - 3.2 + 1) + max(0, -1.7 - 3.2 + 1) = max(0, 2.9) + max(0, -3.9) = 2.9 + 0 = 2.9$로 2.9가 된다. 
마찬가지로 car는 $max(0, 1.3 - 4.9 + 1) + max(0, 2.0 - 4.9 + 1) = max(0, -2.6) + max(0, -1.9) = 0 + 0 = 0$으로 0, 
frog는 $max(0, 2.2 - (-3.1) + 1) + max(0, 2.5 - (-3.1) + 1) = max(0, 6.3) + max(0, 6.6) = 6.3 + 6.6 = 12.0$으로 12.9가 되어 
최종 Loss를 구하면 $L = (2.9 + 0 + 12.9)/3 = 5.27$이 된다. 

1. car 스코어가 조금 변하면 Loss에는 무슨일이 일어날까?
- Loss는 안바뀐다.
2. SVM Loss가 가질 수 있는 최대, 최소값은?
- 최소 0, 최대 무한대
3. 모든 스코어 S가 0과 가깝고, 값이 서로 거의 비슷하다면 Loss는 어떻게 될까?
- 비슷하니 그 차가 마진을 넘지 못하기에 마진값에 가까운 스코어를 얻게 됨. 이경우에서는 결국 (클래스의 수-1)
- 디버깅 전략으로 많이 사용한다. 트레이닝을 처음 시작할 때 Loss가 c-1이 아니면 버그가 있는 것
4. SVM Loss의 경우 정답인 클래스는 제외하고 더했다. 정답인 것도 같이 계산에 포함시킨다면 어떻게 될까?
- Loss 가 1 증가
5. Loss에서 전체 합이 아닌 평균을 쓴다면?
- 영향없다. 단지 스케일만 변할 뿐.
6. 손실함수를 제곱항으로 바꾼다면?
- 비 선형적으로 바뀜. 손실함수의 계산이 달라져 결과가 달라진다. squared hinge loss라 한다.

손실함수의 종류는 많다. 오차를 어떤식으로 해석할지는 우리의 몫이다. 가령 squared hinge loss는 큰 loss를 더 크게, 아주 작은 loss는 더 작게 해준다. 어떤 에러를 trade-off 할지 생각하여 손실함수를 정하게 된다.  

Multiclass SVM loss의 파이썬 코드는 다음과 같다. 
```py
def L_i_vectorized(x, y, w):
  scores = W.dot(x)
  margins = np.maximum(0, scores - scores[y] + 1)
  margins[y] = 0
  loss_i = np.sum(margins)
  return loss_i
```

# Regularization loss
loss가 0인 w를 찾았다고 할 때, 이 w는 유일할까? 그렇지 않다. 2w와 같은 값들도 loss는 0이 된다. 
이유는 2w로 하게 되면 정답 스코어의 차이역시 2배가 될 것이고, w에서 이미 차이가 1보다 크다면 2배를 해도 1보다 크고 loss는 여전히 0이 될 것이기 때문이다.  

loss가 작으면 작을 수록 즉 0이 되면 좋다고만은 할 수 없다. w가 0 이라는 것은 train data에서 완벽한 w라는 것인데 사실상 train set보다는 test 데이터 셋에서의 성능이 더 중요하기 때문이다. 

사실 함수가 단순해야 test 데이터를 맞출 가능성이 더 커지기 때문에 이를 위해 Regularization을 추가해준다. 
따라서 최종적인 Loss의 식은 다음과 같이 표현된다. Data Loss 와 Regularization loss의 합으로 변하고, 하이퍼파라미터인 람다로 두 항간의 트레이드오프를 조절할 수 있다. 

$$L(W) = \frac{1}{N} \sum_{i=1}^N L_i(f(x_i, W), y_i) + \lambda R(W)$$

Regularization에도 다음과 같이 많은 종류가 있다. 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/8fb22bec-c072-48fd-b629-e62ac3d666b6" height="60%" width="60%"></p>

그럼 이 Regularization는 모델이 복잡한지 아닌지, 자신이 원하는 모델인지 어떻게 파악할 수 있을까?

$$ 
\begin{align}
x &= [1,1,1,1]  \\     
w_1 &= [1,0,0,0]  \\    
w_2 &= [0.25,0.25,0.25,0.25]  \\  
\end{align}
$$

다음과 같이 x와 w1,w2가 주어졌고, Linear Classification(f=wx)의 관점으로 볼 때, 두 w는 같은 스코어를 제공한다. 앞서 배운 data loss는 같을 것이다. 
L2와 L1 regression의 관점에서 한 번 살펴보자. 

1. L2 regression의 경우 w2를 선호

$$R(W) = \sum_{k} \sum_{l} W^2_{k, l}$$

$W_2$가 norm이 작기 때문이다. coarse한 것을 고르고 모든 요소가 골고루 영향을 미치길 바란다. (parameter vector, Gaussian prior, MAP inference)

2. L1 regression의 경우 w1을 선호

$$R(W) = \sum_{k} \sum_{l} |W_{k,l}|$$

sparse 한 solution을 고르며 0이 많으면 좋다고 판단한다. 

## Multinomial logistic regression(softmax)
딥러닝에서 자주 쓰이는 다른 유명한 손실함수로 softmax가 있다. 
앞서 Multi-class SVM loss에서는 스코어 자체에 대한 해석보다는 정답 클래스와 정답이 아닌 클래스들을 비교하는 형식으로 이루어졌다.
하지만 Multinomial logistic regression 경우 스코어 자체에 추가적인 의미를 부여한다. 
아래 수식을 가지고 클래스별 확률 분포를 계산하고, 이를 이용해서 Loss를 계산한다. 

$$Softmax \ Function = P(Y = k | X = x_i) = \frac{e^sk}{\sum_{j} e^{s_j}}$$

$$L_i = -log P(Y = y_i | X = x_i)$$





# Reference 
https://wikidocs.net/35476  


