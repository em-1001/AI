# Background
## Cross Entropy
Cross Entropy ëŠ” ì •ë³´ ì´ë¡ ì—ì„œ íŒŒìƒëœ ê°œë…ìœ¼ë¡œ, í™•ë¥  ë¶„í¬ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œì´ë‹¤. ë‘ í™•ë¥  ë¶„í¬ ê°„ì˜ ìœ ì‚¬ì„±ì„ í‰ê°€í•˜ê±°ë‚˜, ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ê°„ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤. 

ë°ì´í„° í™•ë¥  ë¶„í¬ë¥¼ $P(x)$, ëª¨ë¸ì´ ì¶”ì •í•˜ëŠ” í™•ë¥  ë¶„í¬ë¥¼ $Q(x)$ë¼ê³  í•  ë•Œ, ë‘ í™•ë¥  ë¶„í¬ $P$ì™€ $Q$ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œì¸ Cross EntropyëŠ” ì•„ë˜ì™€ ê°™ì´ í‘œí˜„ëœë‹¤. 

$$H(p, q) = H(p) + D_{KL}(p||q) = -\sum_{i=0}^{n} p(x_i)\log{(q(x_i))}$$

ì¼ë°˜ì ì¸ Classification ë¬¸ì œì—ì„œ ì£¼ë¡œ cross entropy lossë¥¼ ì‚¬ìš©í•œë‹¤. ì´ë•Œ True distribution $P$ëŠ” one-hot ì¸ì½”ë”©ëœ vector(Ground Truth)ë¥¼ ì‚¬ìš©í•œë‹¤. Prediction distribution $Q$ ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ ê°’ìœ¼ë¡œ softmax layerë¥¼ ê±°ì¹œ í›„ì˜ ê°’ì´ê³ , í´ë˜ìŠ¤ ë³„ í™•ë¥  ê°’ì„ ëª¨ë‘ í•©ì¹˜ë©´ 1ì´ ëœë‹¤. 

ì˜ˆë¥¼ ë“¤ì–´ $P = [0, 1, 0]$,  $Q = [0.2, 0.7, 0.1]$ ì¼ ë•Œ, cross entropy loss ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

$$
\begin{aligned}
H(P,Q)&=-\sum P(x)log Q(x)\\
&=-(0 \cdot \log{0.2} + 1 \cdot \log{0.7} + 0 \cdot \log{0.1})\\
&=-\log{0.7}
\end{aligned}$$

ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì—ì„œì˜ cross entropyëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤. 

$$H(y, \hat{y}) = -\frac{1}{N} \sum_{i=1}^N (y_i \log{(\hat{y}_i)} + (1-y_i) \log{(1 - \hat{y}_i)})$$

$y_i$ëŠ” ì‹¤ì œ í´ë˜ìŠ¤ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’(0 ë˜ëŠ” 1)ì´ê³ , $\hat{y}_i$ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ ì„ ë‚˜íƒ€ë‚¸ë‹¤.   

ìœ„ ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ìœ ë„ëœë‹¤.
1. ì •ë³´ ì´ë¡ ì  ê´€ì   
ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì—ì„œ, ì‹¤ì œê°’ $y$ê°€ 1ì´ë¼ë©´ ëª¨ë¸ì€ 1ë¡œ ì˜ˆì¸¡í•´ì•¼ í•˜ë©°, cross entropyëŠ” $-log(\hat{y})$ê°€ ë˜ì•¼í•œë‹¤.
ë°˜ëŒ€ë¡œ ì‹¤ì œê°’ $y$ê°€ 0ì´ë¼ë©´ ëª¨ë¸ì€ 0ìœ¼ë¡œ ì˜ˆì¸¡í•´ì•¼ í•˜ë©°, cross entropyëŠ” $-log(1 - \hat{y})$ê°€ ëœë‹¤.
2. í‰ê· í™” ë° í•©ì‚°  
ì´ëŸ¬í•œ ê´€ì ì—ì„œ ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•´ ê°ê°ì˜ ê²½ìš°(ì‹¤ì œê°’ì´ 1 ë˜ëŠ” 0ì¸ ê²½ìš°)ì˜ êµì°¨ ì—”íŠ¸ë¡œí”¼ë¥¼ í•©ì‚°í•˜ì—¬ í‰ê· ì„ ì·¨í•œ ê²ƒì´ ìµœì¢…ì ì¸ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì˜ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜ì˜ ì‹ì´ ëœë‹¤.


ìœ„ ì‹ì€ ì‹¤ì œ ë¶„í¬(Ground Truth)ì¸ $y_i$ì™€ ëª¨ë¸ì˜ ì˜ˆì¸¡ì¸ $\hat{y}_i$ì‚¬ì´ì˜ ì •ë³´ëŸ‰ì„ ì¸¡ì •í•˜ê³  ëª¨ë¸ì´ Ground Truthì™€ ì¼ì¹˜í• ìˆ˜ë¡, Cross Entropyì˜ ê°’ì€ ì‘ì•„ì§„ë‹¤.  

## GIoU, DIoU,  CIoU
ì¼ë°˜ì ìœ¼ë¡œ IoU-based lossëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤. 

$$L = 1 - IoU + \mathcal{R}(B, B^{gt})$$

ì—¬ê¸°ì„œ $R(B, B^{gt})$ëŠ”  predicted box $B$ì™€ target box $B^{gt}$ì— ëŒ€í•œ penalty termì´ë‹¤.  
$1 - IoU$ë¡œë§Œ Lossë¥¼ êµ¬í•  ê²½ìš° boxê°€ ê²¹ì¹˜ì§€ ì•ŠëŠ” caseì— ëŒ€í•´ì„œ ì–´ëŠ ì •ë„ì˜ ì˜¤ì°¨ë¡œ êµì§‘í•©ì´ ìƒê¸°ì§€ ì•Šì€ ê²ƒì¸ì§€ ì•Œ ìˆ˜ ì—†ì–´ì„œ gradient vanishing ë¬¸ì œê°€ ë°œìƒí–ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ penalty termì„ ì¶”ê°€í•œ ê²ƒì´ë‹¤. 

### Generalized-IoU(GIoU)
Generalized-IoU(GIoU) ì˜ ê²½ìš° LossëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤. 

$$L_{GIoU} = 1 - IoU + \frac{|C - B âˆª B^{gt}|}{|C|}$$

ì—¬ê¸°ì„œ $C$ëŠ” $B$ì™€ $B^{gt}$ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” ìµœì†Œ í¬ê¸°ì˜ Boxë¥¼ ì˜ë¯¸í•œë‹¤. Generalized-IoUëŠ” ê²¹ì¹˜ì§€ ì•ŠëŠ” ë°•ìŠ¤ì— ëŒ€í•œ gradient vanishing ë¬¸ì œëŠ” ê°œì„ í–ˆì§€ë§Œ horizontalê³¼ verticalì— ëŒ€í•´ì„œ ì—ëŸ¬ê°€ í¬ë‹¤. ì´ëŠ” target boxì™€ ìˆ˜í‰, ìˆ˜ì§ì„ ì„ ì´ë£¨ëŠ” Anchor boxì— ëŒ€í•´ì„œëŠ” $|C - B âˆª B^{gt}|$ê°€ ë§¤ìš° ì‘ê±°ë‚˜ 0ì— ê°€ê¹Œì›Œì„œ IoUì™€ ë¹„ìŠ·í•˜ê²Œ ë™ì‘í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë˜í•œ ê²¹ì¹˜ì§€ ì•ŠëŠ” boxì— ëŒ€í•´ì„œ ì¼ë‹¨ predicted boxì˜ í¬ê¸°ë¥¼ ë§¤ìš° í‚¤ìš°ê³  IoUë¥¼ ëŠ˜ë¦¬ëŠ” ë™ì‘ íŠ¹ì„± ë•Œë¬¸ì— ìˆ˜ë ´ ì†ë„ê°€ ë§¤ìš° ëŠë¦¬ë‹¤. 

### Distance-IoU(DIoU)
GIoUê°€ ë©´ì  ê¸°ë°˜ì˜ penalty termì„ ë¶€ì—¬í–ˆë‹¤ë©´, DIoUëŠ” ê±°ë¦¬ ê¸°ë°˜ì˜ penalty termì„ ë¶€ì—¬í•œë‹¤. 
DIoUì˜ penalty termì€ ë‹¤ìŒê³¼ ê°™ë‹¤. 

$$\mathcal{R}_{DIoU} = \frac{\rho^2(b, b^{gt})}{c^2}$$

$\rho^2$ëŠ” Euclideanê±°ë¦¬ì´ë©° $c$ëŠ” $B$ì™€ $B^{gt}$ë¥¼ í¬í•¨í•˜ëŠ” ê°€ì¥ ì‘ì€ Boxì˜ ëŒ€ê°ì„  ê±°ë¦¬ì´ë‹¤. 

<p align="center"><img src="https://github.com/em-1001/AI/assets/80628552/4abe5f78-388b-459f-a3f4-95e41a5fdb0a" height="30%" width="30%"></p>

DIoU LossëŠ” ë‘ ê°œì˜ boxê°€ ì™„ë²½íˆ ì¼ì¹˜í•˜ë©´ 0, ë§¤ìš° ë©€ì–´ì§€ë©´ $L_{GIoU} = L_{DIoU} \mapsto 2$ê°€ ëœë‹¤. ì´ëŠ” IoUê°€ 0ì´ ë˜ê³ , penalty termì´ 1ì— ê°€ê¹ê²Œ ë˜ê¸° ë•Œë¬¸ì´ë‹¤. Distance-IoUëŠ” ë‘ boxì˜ ì¤‘ì‹¬ ê±°ë¦¬ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¤„ì´ê¸° ë•Œë¬¸ì— GIoUì— ë¹„í•´ ìˆ˜ë ´ì´ ë¹ ë¥´ê³ , ê±°ë¦¬ê¸°ë°˜ì´ë¯€ë¡œ ìˆ˜í‰, ìˆ˜ì§ë°©í–¥ì—ì„œ ë˜í•œ ìˆ˜ë ´ì´ ë¹ ë¥´ë‹¤. 

### Complete-IoU(CIoU)
DIoU, CIoUë¥¼ ì œì•ˆí•œ ë…¼ë¬¸ì—ì„œ ë§í•˜ëŠ” ì„±ê³µì ì¸ Bounding Box Regressionì„ ìœ„í•œ 3ê°€ì§€ ì¡°ê±´ì€ overlap area, central point
distance, aspect ratioì´ë‹¤. ì´ ì¤‘ overlap area, central pointëŠ” DIoUì—ì„œ ì´ë¯¸ ê³ ë ¤í–ˆê³  ì—¬ê¸°ì— aspect ratioë¥¼ ê³ ë ¤í•œ penalty termì„ ì¶”ê°€í•œ ê²ƒì´ CIoUì´ë‹¤. CIoU penalty termëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤. 

$$\mathcal{R}_{CIoU} = \frac{\rho^2(b, b^{gt})}{c^2} + \alpha v$$

$$v = \frac{4}{Ï€^2}(\arctan{\frac{w^{gt}}{h^{gt}}} - \arctan{\frac{w}{h}})^2$$

$$\alpha = \frac{v}{(1 - IoU) + v}$$

$v$ì˜ ê²½ìš° bboxëŠ” ì§ì‚¬ê°í˜•ì´ê³  $\arctan{\frac{w}{h}} = \theta$ì´ë¯€ë¡œ $\theta$ì˜ ì°¨ì´ë¥¼ í†µí•´ aspect ratioë¥¼ êµ¬í•˜ê²Œ ëœë‹¤. ì´ë•Œ $v$ì— $\frac{2}{Ï€}$ê°€ ê³±í•´ì§€ëŠ” ì´ìœ ëŠ” $\arctan$ í•¨ìˆ˜ì˜ ìµœëŒ€ì¹˜ê°€ $\frac{2}{Ï€}$ ì´ë¯€ë¡œ scaleì„ ì¡°ì •í•´ì£¼ê¸° ìœ„í•´ì„œì´ë‹¤. 

$\alpha$ëŠ” trade-off íŒŒë¼ë¯¸í„°ë¡œ IoUê°€ í° boxì— ëŒ€í•´ ë” í° penaltyë¥¼ ì£¼ê²Œ ëœë‹¤. 

CIoUì— ëŒ€í•´ ìµœì í™”ë¥¼ ìˆ˜í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ê¸°ìš¸ê¸°ë¥¼ ì–»ê²Œ ëœë‹¤. ì´ë•Œ, $w, h$ëŠ” ëª¨ë‘ 0ê³¼ 1ì‚¬ì´ë¡œ ê°’ì´ ì‘ì•„ gradient explosionì„ ìœ ë°œí•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì‹¤ì œ êµ¬í˜„ ì‹œì—ëŠ” $\frac{1}{w^2 + h^2} = 1$ë¡œ ì„¤ì •í•œë‹¤. 

$$\frac{\partial v}{\partial w} = \frac{8}{Ï€^2}(\arctan{\frac{w^{gt}}{h^{gt}}} - \arctan{\frac{w}{h}}) \times \frac{h}{w^2 + h^2}$$ 

$$\frac{\partial v}{\partial h} = -\frac{8}{Ï€^2}(\arctan{\frac{w^{gt}}{h^{gt}}} - \arctan{\frac{w}{h}}) \times \frac{w}{w^2 + h^2}$$ 

## QFL, DFL, GFL
One-stage detectorëŠ” ê¸°ë³¸ì ìœ¼ë¡œ object detectionì„ dense classificationê³¼ localization (i.e., bounding box regression)ì„ í†µí•´ì„œ í•œë‹¤. classificationì˜ ê²½ìš° Focal Lossë¡œ ìµœì í™” ë˜ê³ , box locationì€ Dirac delta distributionìœ¼ë¡œ í•™ìŠµëœë‹¤. QFL, DFLë¥¼ ì œì•ˆí•œ ë…¼ë¬¸ì—ì„œ ë§í•˜ëŠ” ê¸°ì¡´ ë°©ì‹ì˜ ë¬¸ì œëŠ” í¬ê²Œ ë‘ ê°€ì§€ì´ë‹¤. 
1. í•™ìŠµ, ì¶”ë¡  ì‹œ quality estimationê³¼ classificationì˜ ë¹„ì¼ê´€ì„±   
í•™ìŠµì‹œ classification score ì™€ centerness(ë˜ëŠ” iou)score ê°€ ë³„ê°œë¡œ í•™ìŠµë˜ì§€ë§Œ inference ì‹œì—ëŠ” nmsì „ì— ë‘ scoreë¥¼ joiní•´ì„œ ì‚¬ìš©(element wise multiplication)í•œë‹¤. ì´ëŸ¬í•œ ë‘ scoreì˜ ë¹„ì¼ê´€ì„±ì€ ì„±ëŠ¥ì €í•˜ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆê³  ë…¼ë¬¸ì—ì„œëŠ” ë‘ scoreë¥¼ train, test ëª¨ë‘ì—ì„œ jointí•´ì£¼ì–´ ë‘˜ ì‚¬ì´ì˜ ìƒê´€ì„±ì„ í¬ê²Œ ê°–ë„ë¡ ìœ ë„í–ˆë‹¤. 
2. Dirac delta distributionì˜ Inflexible  
ê¸°ì¡´ ë°©ì‹ë“¤ì€ positive sample ìœ„ì¹˜ì—ë§Œ box gtë¥¼ í• ë‹¹í•´ regression í•˜ëŠ” ë°©ì‹ì„ ì·¨í•˜ëŠ”ë° ì´ëŠ” dirac delta distributionìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. dirac delta distributionëŠ” ë¬¼ì²´ì˜ occlusion, shadow, blurë“±ìœ¼ë¡œ ì¸í•œ ë¬¼ì²´ ê²½ê³„ ë¶ˆë¶„ëª… ë“±ì˜ ë¬¸ì œë¥¼ ì˜ ì»¤ë²„í•˜ì§€ ëª»í•œë‹¤.

### Quality Focal Loss(QFL)
trainingê³¼ test ì‹œì˜ inconsistencyë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ supervisionì„ ê¸°ì¡´ì˜ one-hot labelì—ì„œ float target $y âˆˆ [0, 1]$ì´ ê°€ëŠ¥í•˜ë„ë¡ softení•˜ì˜€ë‹¤. ì°¸ê³ ë¡œ ì—¬ê¸°ì„œ $y=0$ì€ 0 quality scoreë¥¼ ê°–ëŠ” negative samplesì„, $0 < y â‰¤ 1$ëŠ” target IoU score $y$ë¥¼ ê°–ëŠ” positive samplesì„ ì˜ë¯¸í•˜ê²Œ ëœë‹¤. ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” QFLëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤. 

$$QFL(\sigma) = -|y - \sigma|^{\beta}((1-y)\log{(1 - \sigma)} + y \log{(\sigma)})$$

ê¸°ì¡´ì˜ Focal Lossì˜ ê²½ìš° $\{ 1, 0 \}$ì˜ discrete labelsë§Œ ì§€ì›í•˜ì˜€ì§€ë§Œ QFLì—ì„œ ì‚¬ìš©í•˜ëŠ” ìƒˆë¡œìš´ labelì€ decimalsì„ í¬í•¨í•˜ë¯€ë¡œ ìœ„ì™€ ê°™ì€ ì‹ì´ ë‚˜ì™”ê³  ê¸°ì¡´ FLì—ì„œ ë°”ë€ ë¶€ë¶„ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. 

1. ê¸°ì¡´ cross entropy partì¸ $-\log{(p_t)}$ê°€ complete versionì¸ $-((1-y)\log{(1 - \sigma)} + y \log{(\sigma)})$ë¡œ í™•ì¥ë˜ì—ˆë‹¤.   
2. scaling factorì¸ $(1-p_t)^{\gamma}$ê°€ estimation $\sigma$ì™€ continuous labe $y$ ì‚¬ì´ì˜ absolute distanceì¸ $|y - \sigma|^{\beta}$ë¡œ ë³€ê²½ë˜ì—ˆë‹¤. ($| Â· |$ëŠ” non-negativitë¥¼ ë³´ì¥í•œë‹¤.)

### Distribution Focal Loss
ê¸°ì¡´ bounding box regressionì˜ ê²½ìš° ì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´ Dirac delta distribution $\delta(x - y)$ë¥¼ ì´ìš©í•´ì„œ regressionë˜ì—ˆë‹¤. 
ì´ëŠ” ì£¼ë¡œ fully connected layersë¥¼ í†µí•´ implementedë˜ë©° ì•„ë˜ì™€ ê°™ë‹¤. 

$$y=\int_{-\infty}^{+\infty} \delta(x - y)x \ dx$$

ë…¼ë¬¸ì—ì„œëŠ” Dirac deltaë‚˜ Gaussian ëŒ€ì‹ ì— General distribution $P(x)$ì„ ì§ì ‘ í•™ìŠµí•˜ëŠ” ê²ƒì„ ì œì•ˆí•œë‹¤. label $y$ì˜ ë²”ìœ„ëŠ” $y_0 â‰¤ y â‰¤ y_n, n \in \mathbb{N}^+$ì— ì†í•˜ê³  estimated valueì¸ $\hat{y}$ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•œë‹¤. ë¬¼ë¡  $\hat{y}$ë„ $y_0 â‰¤ \hat{y} â‰¤ y_n$ë¥¼ ë§Œì¡±í•œë‹¤. 

$$\hat{y}=\int_{-\infty}^{+\infty} P(x)x \ dx = \int_{y_0}^{y_n} P(x)x \ dx$$

ì´ë•Œ í•™ìŠµí•˜ëŠ” labelì˜ ë¶„í¬ê°€ ì—°ì†ì ì´ì§€ ì•Šê³  ì´ì‚°ì ì´ë¯€ë¡œ ìœ„ ì‹ì„ ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. 

$$\hat{y} = \sum_{i=0}^{n} P(y_i)y_i$$

ì´ë•Œ intervals $âˆ†$ì€ ê°„ë‹¨í•˜ê²Œ $âˆ† = 1$ë¡œ í•˜ê³ , $\sum P(y_i) = 1$ì„ ë§Œì¡±í•œë‹¤. 
ìœ„ ì‹ì—ì„œ $y_i$ëŠ” object ì¤‘ì‹¬ìœ¼ë¡œë¶€í„° ê° ë³€ê¹Œì§€ì˜ ê±°ë¦¬ì˜ discreteí•œ ê°’ì´ê³  $P(y_i)$ëŠ” ë„¤íŠ¸ì›Œí¬ê°€ ì¶”ë¡ í•œ í˜„ anchorì—ì„œ boundaryê¹Œì§€ì˜ ê±°ë¦¬ê°€ $y_i$ì¼ í™•ë¥ ì´ë‹¤. ë”°ë¼ì„œ DFLì€ object boundary ê¹Œì§€ì˜ ê±°ë¦¬ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¶”ë¡ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê° ê±°ë¦¬ì— ëŒ€í•œ í™•ë¥  ê°’ì´ ìˆìœ¼ë©´ ì´ ê°’ë“¤ì˜ ê¸°ëŒ“ê°’ìœ¼ë¡œ ì¶”ë¡ í•˜ëŠ” ê²ƒì´ë‹¤. 

$P(x)$ëŠ” softmax $S(\cdot)$ì„ í†µí•´ ì‰½ê²Œ êµ¬í•´ì§ˆ ìˆ˜ ìˆë‹¤. ë˜í•œ $P(y_i)$ë¥¼ ê°„ë‹¨í•˜ê²Œ $S_i$ë¡œ í‘œí˜„í•œë‹¤. DFLì„ í†µí•œ í•™ìŠµì€ $P(x)$ì˜ í˜•íƒœê°€ targetì¸ $y$ì— ê°€ê¹Œìš´ ê°’ì´ ë†’ì€ probabilitiesë¥¼ ê°–ë„ë¡ ìœ ë„í•œë‹¤. ë”°ë¼ì„œ DFLì€ target $y$ì— ê°€ì¥ ê°€ê¹Œìš´ ë‘ ê°’ $y_i, y_{i+1}$ ($y_i â‰¤ y â‰¤ y_{i+1}$)ì˜ probabilitiesë¥¼ ë†’ì„ìœ¼ë¡œì„œ ë„¤íŠ¸ì›Œí¬ê°€ ë¹ ë¥´ê²Œ label $y$ì— ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. 
DFLì€ QFLì˜ complete cross entropy partë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤. 

$$DFL(S_i, S_{i+1}) = -((y_{i+1} - y) \log{(S_i)} + (y - y_i) \log{(S_{i+1})})$$

DFLì˜ global minimum solutionì€ $S_i = \frac{y_{i+1} - y}{y_{i+1} - y_i}, \ S_{i+1} = \frac{y - y_i}{y_{i+1} - y_i}$ê°€ ë˜ê³  ì´ë¥¼ í†µí•´ ê³„ì‚°í•œ estimated regression target $\hat{y}$ëŠ” corresponding labe $y$ì— ë¬´í•œíˆ ê°€ê¹ë‹¤.    
i.e $\hat{y} = \sum P(y_j)y_j = S_i y_i + S_{i+1} y_{i+1} = \frac{y_{i+1} - y}{y_{i+1} - y_i} y_i + \frac{y - y_i}{y_{i+1} - y_i} y_{i+1} = y$

## YOLOv1
YOLOv1ì´ ì‚¬ìš©í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ì— ì´ë¯¸ì§€ë¥¼ í†µê³¼ì‹œí‚¤ë©´ ê²°ê³¼ë¡œ SxS ê·¸ë¦¬ë“œ ì…€ì˜ í´ë˜ìŠ¤ í™•ë¥  Cì™€ ì˜ˆì¸¡ëœ ë°”ìš´ë”© ë°•ìŠ¤ B, ê·¸ë¦¬ê³  Confidence Scoreê°€ ì£¼ì–´ì§„ë‹¤. ì—¬ê¸°ì„œ SxSë¡œ ë‚˜ëˆˆ ê·¸ë¦¬ë“œ ì…€ ì¤‘ ë¬¼ì²´ì˜ ì¤‘ì•™ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ì…€ì´ ê°ì²´ë¥¼ íƒì§€í•˜ëŠ” ì—­í• ì„ í•˜ê²Œëœë‹¤. ê·¸ë¦¬ê³  ê° ì…€ì€ ë°”ìš´ë”© ë°•ìŠ¤ Bì™€ ë¶„ë¥˜í•œ í´ë˜ìŠ¤ì˜ í™•ë¥ ì¸ Cë¥¼ ì˜ˆì¸¡í•œë‹¤. 

**ë°”ìš´ë”© ë°•ìŠ¤ B** ëŠ” X, Y ì¢Œí‘œ, ê°€ë¡œ, ì„¸ë¡œ í¬ê¸° ì •ë³´ì™€ Confidence Score (Score)ìˆ˜ì¹˜ë¥¼ ê°€ì§€ê³  ìˆë‹¤. ScoreëŠ” Bê°€ ë¬¼ì²´ë¥¼ ì˜ì—­ìœ¼ë¡œ ì¡ê³  ìˆëŠ”ì§€ì™€ í´ë˜ìŠ¤ë¥¼ ì˜ ì˜ˆì¸¡í•˜ì˜€ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Scoreë¥¼ ê°„ë‹¨í•˜ê²Œ **Pr(Object) âˆ— IOU** ë¡œ ì •ì˜í•˜ê³  ìˆëŠ”ë°, **Pr(Object)** ëŠ” ë°”ìš´ë”© ë°•ìŠ¤ ì•ˆì— ë¬¼ì²´ê°€ ì¡´ì¬í•  í™•ë¥ ì´ë‹¤. ë§Œì•½ ë°”ìš´ë”© ë°•ìŠ¤ê°€ ë°°ê²½ë§Œì„ ì˜ì—­ìœ¼ë¡œ ì¡ê³  ìˆë‹¤ë©´ Pr(Object)ì˜ ê°’ì´ 0ì´ë¯€ë¡œ ScoreëŠ” 0ì´ëœë‹¤.

**í´ë˜ìŠ¤ í™•ë¥  C** ëŠ” ê·¸ë¦¬ë“œ ì…€ ì•ˆì— ìˆëŠ” ê·¸ë¦¼ì˜ ë¶„ë¥˜ í™•ë¥ ì„ ë‚˜íƒ€ë‚¸ë‹¤. ê¸°í˜¸ë¡œëŠ” **Pr(Class_i |Object)** ë¡œ í‘œí˜„í•˜ë©° Bê°€ ë°°ê²½ì´ ì•„ë‹Œ ê°ì²´ë¥¼ í¬í•¨í•˜ëŠ” ê²½ìš°ì˜ ê° í´ë˜ìŠ¤ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì´ë‹¤. Bê°€ ë°°ê²½ì„ ì˜ˆì¸¡í–ˆë‹¤ë©´ í™•ë¥ ì€ 0ì´ ëœë‹¤. ìµœì¢…ì ìœ¼ë¡œ í´ë˜ìŠ¤ ì¡°ê±´ë¶€ í™•ë¥  Cì™€ ê° ë°”ìš´ë”© ë°•ìŠ¤ì˜ Confidence ì˜ˆì¸¡ ê°’ì„ ê³±í•˜ë©´ ê° ë°•ìŠ¤ì˜ í´ë˜ìŠ¤ë³„ Confidence Score ìˆ˜ì¹˜ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.

$$Pr(Class_i |Object) * Pr(Object) * IOU = Pr(Class_i) * IOU$$

### YOLOv1 Loss Function
YOLOv1ì€ Training Networkë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì„¤ê³„í•˜ê¸° ì „ ë‹¤ìŒê³¼ ê°™ì€ ì›ì¹™ì„ ë§Œë“¤ì—ˆë‹¤. 

1. ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” classifier ë¬¸ì œë¥¼ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ë§Œë“œëŠ” regressionë¬¸ì œë¡œ ìƒê°í•œë‹¤.
2. ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì˜ ê·¸ë ¸ëŠ”ì§€ í‰ê°€í•˜ëŠ” Localization Errorì™€ ë°•ìŠ¤ ì•ˆì˜ ë¬¼ì²´ë¥¼ ì˜ ë¶„ë¥˜í–ˆëŠ”ì§€ í‰ê°€í•˜ëŠ” Classification Errorì˜ íŒ¨ë„í‹°ë¥¼ ë‹¤ë¥´ê²Œ í‰ê°€í•œë‹¤. íŠ¹íˆ, ë°•ìŠ¤ ì•ˆì˜ ë¬¼ì²´ê°€ ì—†ëŠ” ê²½ìš°ì—ëŠ” Confidence Scoreë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•´ Localization Error ì— ë” ë†’ì€ íŒ¨ë„í‹°ë¥¼ ë¶€ê³¼í•œë‹¤.
3. ë§ì€ ë°”ìš´ë”© ë°•ìŠ¤ì¤‘ì— IOU ìˆ˜ì¹˜ê°€ ê°€ì¥ ë†’ê²Œ ìƒì„±ëœ ë°”ìš´ë”© ë°•ìŠ¤ë§Œ í•™ìŠµì— ì°¸ì—¬í•œë‹¤. ì´ëŠ” ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì˜ ë§Œë“œëŠ” ì…€ì€ ë”ìš± í•™ìŠµì„ ì˜í•˜ë„ë¡ ë†’ì€ Confidence Scoreë¥¼ ì£¼ê³  ë‚˜ë¨¸ì§€ ì…€ì€ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì˜ ë§Œë“¤ì§€ ëª»í•˜ë”ë¼ë„ ë‚˜ì¤‘ì— Non-max suppressionì„ í†µí•´ ìµœì í™” í•˜ê¸° ìœ„í•¨ì´ë‹¤.

YOLOëŠ” 1ë²ˆ ì›ì¹™ì„ ì§€í‚¤ê¸° ìœ„í•´ Loss Function ì—ì„œ **Sum-Squared Error(SSD)** ë¥¼ ì´ìš©í•œë‹¤. ê·¸ë¦¬ê³  2ë²ˆ ì›ì¹™ì„ ë§Œì¡±í•˜ê¸° ìœ„í•´ì„œ $Î»_{coord}$ ì™€ $Î»_{noobj}$ ë‘ ê°œì˜ ë³€ìˆ˜ë¥¼ ì´ìš©í•œë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” $Î»_{coord} = 5, Î»_{noobj} = 0.5$ ë¡œ ì„¤ì •í•˜ì˜€ë‹¤. ì•„ë˜ëŠ” YOLOv1ì˜ Loss Functionì´ë‹¤. 



$$Î»_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^B ğŸ™^{obj}_{i j} \left[(x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 \right]$$

$$+Î»_{coord} \sum_{i=0}^{S^2} \sum_{j=0}^B ğŸ™^{obj}_{i j} \left[(\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2 \right]$$

$$+\sum_{i=0}^{S^2} \sum_{j=0}^B ğŸ™^{obj}_{i j} \left(C_i - \hat{C}_i\right)^2$$

$$+Î»_{noobj} \sum_{i=0}^{S^2} \sum_{j=0}^B ğŸ™^{noobj}_{i j} \left(C_i - \hat{C}_i\right)^2$$

$$+\sum_{i=0}^{S^2} ğŸ™^{obj}_ {i} \sum_{c \in classes} (p_i(c) - \hat{p}_i(c))^2$$  

$S$ : ê·¸ë¦¬ë“œ ì…€ì˜ í¬ê¸°ë¥¼ ì˜ë¯¸í•œë‹¤. í–‰ë ¬ ì´ê¸° ë•Œë¬¸ì— ì „ì²´ ê·¸ë¦¬ë“œ ì…€ì˜ ê°œìˆ˜ëŠ” SÂ² ê°€ ëœë‹¤.      
$B$ : $S_i$ ì…€ì˜ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì˜ë¯¸í•œë‹¤.    
$C$ : ê° ê·¸ë¦¬ë“œ ì…€ì´ êµ¬ë¶„í•œ í´ë˜ìŠ¤ì™€ ê°™ë‹¤.   
$Î»_{coord}$ : 5ë¡œ ì„¤ì •ëœ Î»_coord ë³€ìˆ˜ë¡œì„œ Localization ì—ëŸ¬ì— 5ë°° ë” ë†’ì€ íŒ¨ë„í‹°ë¥¼ ë¶€ì—¬í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•œë‹¤.    
$ğŸ™^{obj}_ {i j}$ : ië²ˆì§¸ ì…€ì˜ jë²ˆ ë°”ìš´ë”© ë°•ìŠ¤ë§Œì„ í•™ìŠµí•˜ê² ë‹¤ëŠ” ì˜ë¯¸ë¡œ ì‚¬ìš©í•˜ì§€ë§Œ ëª¨ë“  ì…€ì— ëŒ€í•´ì„œ ë°”ìš´ë”© ë°•ìŠ¤ í•™ìŠµì´ ì¼ì–´ë‚˜ì§€ ì•Šê³  ê° ê°ì²´ë§ˆë‹¤ IOUê°€ ê°€ì¥ ë†’ì€ ë°”ìš´ë”© ë°•ìŠ¤ì¸ ê²½ìš°ì—ë§Œ íŒ¨ë„í‹°ë¥¼ ë¶€ê³¼í•´ì„œ í•™ìŠµì„ ë” ì˜í•˜ë„ë¡ ìœ ë„í•œë‹¤.  
$Î»_{noobj}$ : í•´ë‹¹ ì…€ì— ê°ì²´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°, ì¦‰ ë°°ê²½ì¸ ê²½ìš°ì—ëŠ” ë°”ìš´ë”© ë°•ìŠ¤ í•™ìŠµì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šë„ë¡ 0.5ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³±í•´ì£¼ì–´ì„œ íŒ¨ë„í‹°ë¥¼ ë‚®ì¶˜ë‹¤.    
$ğŸ™^{noobj}_{i j}$ : ië²ˆì§¸ ì…€ê³¼ jë²ˆì§¸ ë°”ìš´ë”© ë°•ìŠ¤ì— ê°ì²´ê°€ ì—†ëŠ” ê²½ìš°ì— ìˆ˜í–‰ í•œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.

3,4 ë²ˆì§¸ í•­ì€ ê°ê° bouding boxê°€ ê°ì²´ë¥¼ í¬í•¨í•  ë•Œì™€ ë°°ê²½ì¼ ë•Œì˜ confidence errorë¥¼ ê³„ì‚°í•˜ê³  ë§ˆì§€ë§‰ 5ë²ˆì§¸ í•­ì€ bouding boxì™€ ê´€ê³„ì—†ì´ ê° ì…€ë§ˆë‹¤ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•œ ì˜¤ì°¨ì´ë‹¤. 



# Real-Time Flying Object Detection with YOLOv8
ë³¸ ë…¼ë¬¸ì€ í˜„ì¬ state-of-the-artì¸ YOLOv8ì„ ì´ìš©í•œ ë¹„í–‰ì²´ íƒì§€ëª¨ë¸ì„ ì œì•ˆí•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ Real-time object detectionì€ objectì˜ ê³µê°„ì  ì‚¬ì´ì¦ˆ(spatial sizes), ì¢…íš¡ë¹„(aspect ratios), ëª¨ë¸ì˜ ì¶”ë¡  ì†ë„(inference
speed), ê·¸ë¦¬ê³  noise ë“±ì˜ ë³€ìˆ˜ë¡œ ì–´ë ¤ì›€ì´ ìˆì—ˆë‹¤. ë¹„í–‰ì²´ëŠ” ìœ„ì¹˜(location), í¬ê¸°(scale), íšŒì „(rotation), ê¶¤ë„(trajectory)ê°€ ë§¤ìš° ë¹ ë¥´ê²Œ ë³€í•˜ê¸° ë•Œë¬¸ì— ì•ì„  ë¬¸ì œë“¤ì€ ë¹„í–‰ì²´ë¥¼ íƒì§€í•˜ëŠ”ë° ë”ìš± ë¶€ê°ëœë‹¤. ê·¸ë ‡ê¸°ì— ë¹„í–‰ì²´ì˜ ì´ëŸ¬í•œ ë³€ìˆ˜ì— ëŒ€í•´ thoroughí•˜ê³  ë¹ ë¥¸ ì¶”ë¡ ì†ë„ë¥¼ ê°–ëŠ” ëª¨ë¸ì´ ì¤‘ìš”í–ˆë‹¤. 

<img src="https://github.com/em-1001/AI/assets/80628552/7c8e5c53-3e12-46fa-813f-6698c1b06538" height="80%" width="80%">

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” datasetì¤‘ 80%ë¥¼ train, 20%ì„ validationìœ¼ë¡œ ë‚˜ëˆ„ì—ˆë‹¤. ê° datasetì˜ ì´ë¯¸ì§€ëŠ” class numberê°€ labelë˜ì–´ìˆê³ , bounding box ê°€ì¥ìë¦¬ì˜ ì¢Œí‘œë¥¼ í‘œì‹œí•´ë†¨ë‹¤. í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì—ëŠ” í‰ê· ì ìœ¼ë¡œ 1.6ê°œì˜ objectê°€ ìˆê³ , median image
ratioëŠ” 416x416ì´ë‹¤. ì´ë¯¸ì§€ëŠ” auto orientationìœ¼ë¡œ ì „ì²˜ë¦¬ ë˜ì—ˆìœ¼ë©°, augmentationsì€ ì ìš©í•˜ì§€ ì•Šì•˜ë‹¤. 

##  Model Choice and Evaluation
ë…¼ë¬¸ì—ì„œëŠ” ìš°ì„  YOLOv8ì˜ small, medium, and large ë²„ì „ì— ëŒ€í•´ ìµœì ì˜ inference speedì™€ mAP50-95ë¥¼ ê°–ëŠ” ëª¨ë¸ ë²„ì „ì„ ì„ íƒí–ˆê³ , ì´í›„ì—  hyper parametersë¥¼ ìµœì í™” í–ˆë‹¤. 
VOLOv8ì˜ ê° ë²„ì „ì— ëŒ€í•œ ì •ë³´ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.   
nano (yolov8n), small (yolov8s), medium (yolov8m), large (yolov8l), and extra large (yolov8x)

|Model|Input size(pixels)|mAP50-95|params(M)|FLOPS(B)|
|-|-|-|-|-|
|YOLOv8n|640|37.3|3.2|8.7|
|YOLOv8s|640|44.9|11.2|28.6|
|YOLOv8m|640|50.2|25.9|78.9|
|YOLOv8l|640|52.9|43.7|165.2|
|YOLOv8x|640|53.9|68.2|257.8|

small, medium, large ëª¨ë¸ë“¤ì˜ parametersëŠ” ê°ê° (11151080, 25879480, & 43660680)ì´ê³ , layersëŠ” (225,295, & 365)ì´ë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œ í•™ìŠµì‹œí‚¨ ê²°ê³¼ smallê³¼ medium ì‚¬ì´ì—ì„œëŠ” mAP50-95ì—ì„œ í° ì„±ëŠ¥í–¥ìƒì´ ìˆì—ˆìœ¼ë‚˜ medium, large ì‚¬ì´ì—ì„œëŠ” ê·¸ë ‡ì§€ ì•Šì•˜ë‹¤ê³  í•œë‹¤. ë˜í•œ validation setì— ëŒ€í•´ì„œ small, medium, and largeì˜ inference speedëŠ” ê°ê° 4.1, 5.7, and 9.3 milliseconds ì˜€ë‹¤ê³  í•œë‹¤. ì›ë˜ ëª©í‘œì˜€ë˜ average inference speedëŠ” 30 to 60 frames for 1080pì˜€ê³ , medium size
modelì„ multiple 1080p HD videosì—ì„œ í…ŒìŠ¤íŠ¸í•´ë³¸ ê²°ê³¼ average total speed (pre-proccess speed(0.5ms) + inference speed(17.25ms) + post-process speed(2ms)) of 19.75 ms(50 frames per second)ë¡œ ëª©í‘œì— ì í•©í•˜ì—¬ ëª¨ë¸ì„ medium sizeë¡œ ê²°ì •í•˜ê³  hyper-parameters íŠœë‹ì„ ì§„í–‰í–ˆë‹¤ê³  í•œë‹¤. 

## Loss Function and Update Rule
ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” Loss Functionì„ ì¼ë°˜í™”í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤. 

$$L(Î¸) = \frac{Î»_{box}}{N_{pos}}L_{box}(Î¸) + \frac{Î»_{cls}}{N_{pos}}L_{cls}(Î¸) + \frac{Î»_{dfl}}{N_{pos}}L_{dfl}(Î¸) + Ï†||Î¸||^2_2$$

$$V^t = \beta V^{t-1} + âˆ‡_Î¸ L(Î¸^{t-1})$$

$$Î¸^t = Î¸^{t-1} - Î·V^t$$

ì²« ë²ˆì§¸ ì‹ì€ ì¼ë°˜í™”ëœ Loss Functionìœ¼ë¡œ box loss, classification loss, distribution focal loss ê°ê°ì˜ Lossë“¤ì„ í•©í•˜ê³ , weight decayì¸ $Ï†$ë¥¼ í™œìš©í•´ ë§ˆì§€ë§‰ í•­ì—ì„œ regularizationì„ í•œë‹¤. ë‘ ë²ˆì§¸ ì‹ì€ momentum $Î²$ë¥¼ ì´ìš©í•œ velocity termì´ë‹¤. ì„¸ ë²ˆì§¸ ì‹ì€ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ë¡œ $Î·$ëŠ” learning rateì´ë‹¤. 

YOLOv8ì˜ loss functionì„ ìì„¸íˆ ì‚´í´ë³´ë©´ ì•„ë˜ì™€ ê°™ë‹¤. 

$$L = \frac{Î»_ {box}}{N_ {pos}} \sum_ {x, y} ğŸ™_ {c^{\star}_ {x, y}} \left[1 - q_ {x,y} + \frac{||b_ {x, y} - \hat{b}_ {x, y}||^2_2}{Ï^2} + Î±_ {x, y} v_ {x, y}\right]$$

$$+\frac{Î»_ {cls}}{N_ {pos}} \sum _{x,y} \sum _{c \in classes} y _c log(\hat{y} _c) + (1 - y _c) log(1 - \hat{y} _c)$$ 

$$+\frac{Î»_{dfl}}{N_{pos}} \sum_{x,y} ğŸ™_{c^{\star}_ {x, y}} - \left[(q_ {(x,y)+1} - q_{x,y})log(\hat{q}_ {x,y}) + (q_{x,y} - q_{(x,y)-1})log(\hat{q}_{(x,y)+1})\right]$$

$where:$

$$q_{x,y} = IOU_{x,y} = \frac{\hat{Î²}_ {x,y} âˆ© Î²_{x,y}}{\hat{Î²}_ {x,y} âˆª Î²_{x,y}}$$

$$v_{x,y} = \frac{4}{Ï€^2}(\arctan{(\frac{w_{x,y}}{h_{x,y}})} - \arctan{(\frac{\hat{w}_ {x,y}}{\hat{h}_{x,y}})})^2$$

$$Î±_{x,y} = \frac{v}{1 - q_{x,y}}$$

$$\hat{y}_c = Ïƒ(Â·)$$

$$\hat{q}_{x,y} = softmax(Â·)$$

$and:$


box loss : https://arxiv.org/abs/1911.08287  
class loss : standard binary cross entropy  
distribution focal loss :  https://arxiv.org/abs/2006.04388  


# Reference 
## Web Link 
One-stage object detection : https://machinethink.net/blog/object-detection/  
YOLOv5 : https://blog.roboflow.com/yolov5-improvements-and-evaluation/   
YOLOv8 : https://blog.roboflow.com/whats-new-in-yolov8/       
mAP : https://blog.roboflow.com/mean-average-precision/    
SiLU : https://tae-jun.tistory.com/10     
Weight Decay, BN : https://blog.janestreet.com/l2-regularization-and-batch-norm/  
Focal Loss : https://gaussian37.github.io/dl-concept-focal_loss/  
ã€€ã€€ã€€ã€€ ã€€https://woochan-autobiography.tistory.com/929  
Cross Entropy : https://sosoeasy.tistory.com/351  
DIOU, CIOU : https://hongl.tistory.com/215    
QFL, DFL : https://pajamacoder.tistory.com/m/74  
YOLOv8 Loss ìˆ˜ì • : https://velog.io/@easyssun/YOLOv8-%EB%AA%A8%EB%8D%B8-loss-function-%EC%88%98%EC%A0%95  

## Paper 
Real-Time Flying Object Detection with YOLOv8 : https://arxiv.org/pdf/2305.09972.pdf   
YOLO : https://arxiv.org/pdf/1506.02640.pdf    
YOLOv2 : https://arxiv.org/pdf/1612.08242.pdf    
YOLOv3 : https://arxiv.org/pdf/1804.02767.pdf  
YOLOv4 : https://arxiv.org/pdf/2004.10934.pdf   
YOLOv6 : https://arxiv.org/pdf/2209.02976.pdf  
YOLOv7 : https://arxiv.org/pdf/2207.02696.pdf  
